{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6 Quasi-hyperbolic discounting and self-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note augments the environment studied in exercise 7.6 on Self-control of Ljungqvist and Sargent Recursive Macro textbook to an environment with productivity shocks. It is a partial equilibrium model of a present-biased household. The equilibrium concept is Markov Perfect Equilibrium. First, we revise the equilibrium concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Markov Perfect Equilibrium\n",
    "\n",
    "Consider an economy with:\n",
    "- 2 agents with utility $u_i(a_i, a_j, x)$ where $a_i$ denotes the action of $i$ and $x$ is the state variable.\n",
    "- the future is discounted at rate $\\beta$ so each agent, taking the strategy profile $a_{j}$ as given, solves:\n",
    "\\begin{align*}\n",
    "\\max_{(a_{it})_{t=0}^\\infty} &\\ \\sum_{t=0}^\\infty \\beta^t \\ u_i(a_i, a_j(h_t), x_t)\\\\\n",
    " & h_{t+1} = (h_t, (a_{it}, a_{jt}(h_t)))\\\\\n",
    " & x_{t+1} = f(a_{it}, a_{jt}(h_t), x_t)\n",
    "\\end{align*}\n",
    "Note that each player takes into account the effect that her action has on the history and hence on the action of the other player but a player takes the strategy (not the sequence of actions) of the other player as given and unaffected by her choice of srategies.\n",
    "- an action profile is Markovian if for every $i$, $a_i(x_t, h_{t-1}) = a_i(x_t, \\tilde{h}_{t-1})$ for all histories $\\tilde{h}_{t-1}$.\n",
    "- the Markovian best response of an agent can be formulated in recursive form:\n",
    "\\begin{align*}\n",
    "v_i(x) = \\max_{a_{i}} & \\ u_i(a_i, a_j(x), x) + \\beta v_i(x')\\\\\n",
    " & x' = f(a_i, a_j(x), x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Perfect Equilibrium is a pair of value functions $(v_1, v_2)$ and Markovian best responses $(a_1, a_2)_{i=1,2}$ such that $v_i$ solves the functional equation for agent $i$ and $a_i$ is the associated policy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above problems are difficult to solve because they involve interrelated value functions iterations.\n",
    "The FOC are, for all $x$:\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial u_i(a_i, a_j(x), x)}{\\partial a_i} + \\beta v_i'(f(a_1, a_2, x)) \\frac{\\partial f(a_i, a_j(x), x)}{\\partial a_i} = 0\n",
    "\\end{equation*}\n",
    "and by Benveniste-Scheinckman,\n",
    "\\begin{equation*}\n",
    "v_i'(x') = \\frac{\\partial u_i(a_i, a_j(x'), x')}{\\partial a_j} \\frac{\\partial a_j}{\\partial x} + \\frac{\\partial u_i(a_i, a_j(x'), x')}{\\partial x} \n",
    "\\end{equation*}\n",
    "\n",
    "If the problem is linear quadratic, then the interrelated Belleman equation becomes stacked Riccati equations which is easy to solve with the method of undetermined coefficients. This is particularly useful for oligopoli models for instance. Below we solve a single planner problem who has time-inconsistent preferences due to a present-bias modelled with quasi-hyperbolic (also known as $\\beta \\delta$) preferences. The time-inconsistency in preferences makes it a game played between different selves of the same planner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present-biased household\n",
    "\n",
    "- finite horizon $T$\n",
    "- representative household (with $T+1$ selves)\n",
    "- inelastic labor normalized to $1$\n",
    "- one sector model with full depreciation\n",
    "\\begin{equation}\n",
    "c_t + k_{t+1} = \\xi_t f(k_t)\n",
    "\\end{equation}\n",
    "with $f' > 0$ and $f'' \\leq 0$ and $\\xi_t \\sim_{iid} \\phi$.\n",
    "\n",
    "Preferences are:\n",
    "- at $T$: \n",
    "\\begin{equation}\n",
    "u(c_T)\n",
    "\\end{equation}\n",
    "- at $T-1$: \n",
    "\\begin{equation}\n",
    "u(c_{T-1}) + \\beta \\delta E[u(c_T)]\n",
    "\\end{equation}\n",
    "- at $T-2$: \n",
    "\\begin{equation}\n",
    "u(c_{T-2}) + \\beta E[\\delta u(c_{T-1}) + \\delta^2 u(c_T)]\n",
    "\\end{equation}\n",
    "- etc\n",
    "\n",
    "Note that the household has time-inconsistent preferences (i.e. not recursive): the time $t-$self discounts the next period with factor $\\beta \\delta$ and any two subsequent periods after that with factor $\\delta$.\n",
    "\n",
    "The household chooses $c_t$ at time $t$ and cannot commit to future choices. A household that can commit to future choices is said to have ``self-control''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remarks:\n",
    "- finite horizon limits the set of possible equilibria. With infinite horizon, folks theorems suggest that we could support more equilibria (including self-control) with trigger strategies\n",
    "- $T+1$ personalities makes us model the situation as a game played by different selves of the household"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is obtained by backward induction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "u(f(k_T)) \\tag{period $T$}\\\\\n",
    "\\max_k u(f(k_{T-1}) - k) + \\beta \\delta E[u(f(k))] \\tag{period $T-1$}\\\\\n",
    "\\max_k u(f(k_{T-2}) - k) + \\beta E[\\delta u(f(k_{T-1}(k))) + \\delta^2 u(f(k_{T}(k_{T-1}(k))))] \\tag{period $T-1$}\n",
    "\\end{align}\n",
    "\n",
    "The sequential formulation makes it clear that the current self anticipates the effect of its choice on the choice of future selves, taking the strategy of future selves as given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive formulation\n",
    "\n",
    "Different selves disagree on discounting. Each period, the value function $v_{T-t}$ records the value function of the period $T-t$-self and the value function $W_{T-t}$ record the continuation value for all other selves. With a finite horizon, we have a sequence of value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Period $T$ is the last period:\n",
    "\\begin{align}\n",
    "v_T(y) &= u(y)\\\\\n",
    "W_T(y) &= u(y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index $t$ \n",
    "\n",
    "\\begin{align}\n",
    "v_{T-t}(y) &:= \\max_{k'} u(y - k') + \\beta  \\delta E[W_{T-t+1}(\\xi f(k'))]  \\\\\n",
    "k_{T-t}'(k)&:= \\arg\\max_{k'} u(y - k') + \\beta  \\delta E[W_{T-t+1}(\\xi f(k'))]  \\\\\n",
    "W_{T-t}(y) &:= u(y - k_{T-t}'(k)) + \\delta E[W_{T-t+1}(\\xi f(k_{T-t}'(k)))]\n",
    "\\end{align}\n",
    "where $k_{T-t}'(k)$ denotes the policy function from the right hand side the equation defining $v_{T-t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quantecon as qe\n",
    "import matplotlib.pyplot as plt\n",
    "from interpolation import interp\n",
    "from numba import jit, njit, jitclass, prange, float64, int32\n",
    "from quantecon.optimize.scalar_maximization import brent_max\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'This is to be able to use JIT from Numba to speed up the code, which will be done with at jitclass before the class'\n",
    "qhyp_cons = [\n",
    "    ('alpha', float64), # power of production function\n",
    "    ('beta', float64), # inverse measure of present bias\n",
    "    ('delta', float64), # discount factor            \n",
    "    ('mu', float64),          # Shock location parameter\n",
    "    ('s', float64),          # Shock scale parameter\n",
    "    ('grid', float64[:]),    # Grid (array)\n",
    "    ('shocks', float64[:]),  # Shock draws (array)\n",
    "    ('T', int32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass(qhyp_cons) #jitclass because we are building a class\n",
    "class LS_qhyp_cons():\n",
    "    def __init__(self,\n",
    "                alpha = 0.4,\n",
    "                beta= 0.96,\n",
    "                delta = 0.96,\n",
    "                mu = 0,\n",
    "                s = 0.1,\n",
    "                grid_max = 4,\n",
    "                grid_size = 200,\n",
    "                shock_size = 250,\n",
    "                T = 10):\n",
    "        self.alpha, self.beta, self.delta, self.mu, self.s, self.T = alpha, beta, delta, mu, s, T\n",
    "        self.grid = np.linspace(1e-5, grid_max, grid_size) # set up the grid for the state y\n",
    "        self.shocks = np.exp(mu + s * np.random.randn(shock_size)) # draw some shocks to take expectations\n",
    "    \n",
    "    def f(self, k):\n",
    "        return k**self.alpha\n",
    "    \n",
    "    def f_prime(self, k):\n",
    "        return self.alpha * k**(self.alpha - 1)\n",
    "    \n",
    "    def u(self, c):\n",
    "        return np.log(c)\n",
    "    \n",
    "    def u_prime(self, c):\n",
    "        return 1/c\n",
    "    \n",
    "    def RHS_Bellman(self, c, y, w_array):\n",
    "        'returns the right hand side for a given choice c and a given state y'\n",
    "        u, f, beta, delta, shocks = self.u, self.f, self.beta, self.delta, self.shocks\n",
    "        w = lambda x: interp(self.grid, w_array, x)\n",
    "        return u(c) + beta * delta * np.mean(w(f(y-c) * shocks)) #np.mean computes the expectation over shock_size draws\n",
    "    \n",
    "    def Update_cont_val(self, c, y, w_array):\n",
    "        u, f, beta, delta, shocks = self.u, self.f, self.beta, self.delta, self.shocks\n",
    "        w = lambda x: interp(self.grid, w_array, x)\n",
    "        return u(c) + delta * np.mean(w(f(y-c) * shocks)) #np.mean computes the expectation over shock_size draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operator $v\\_k\\_w(W)$ computes, for each output $y$ on a grid, the current policy function $k'(\\cdot)$, the associated value function $v$ for period $t$-self and the continuation value $w$ from the perspective of all other selves:\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "v \\\\\n",
    "k \\\\\n",
    "W\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "u(y - k'(k)) + \\beta  \\delta E[W(\\xi f(k'(k)))] \\\\\n",
    "k'(k) \\ \\text{policy function v-greedy} \\\\\n",
    "u(y - k'(k)) + \\delta E[W(\\xi f(k'(k)))]\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "This operator computes . It also computes the continuation value function with geometric discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit #(nopython=True) # jit because we are defining a function\n",
    "def v_k_w(qh, W):\n",
    "    ''' \n",
    "    qh is an instance of the class LS_qhyp_cons\n",
    "    W is the value function discounted by delta, an array of size grid_size in the class LS_qhyp_cons\n",
    "    '''\n",
    "    v_previous = np.empty_like(W) # creates a vector of the same size as W that will record v_previous\n",
    "    w_previous = np.empty_like(W)\n",
    "    c_greedy = np.empty_like(W)\n",
    "    v_k_w_sol = np.empty((3, len(W)))\n",
    "    for i in range(len(qh.grid)):\n",
    "        y = qh.grid[i]\n",
    "        c_val, vprevious, info = brent_max(qh.RHS_Bellman, 1e-10, y, args=(y, W)) # maximize over c between 0 and y-1e-10\n",
    "        v_previous[i] = vprevious\n",
    "        c_greedy[i] = c_val\n",
    "        w_previous[i] = qh.Update_cont_val(c_val, y, W)\n",
    "        v_k_w_sol[0,:] = v_previous\n",
    "        v_k_w_sol[1,:] = c_greedy\n",
    "        v_k_w_sol[2,:] = w_previous\n",
    "    return v_k_w_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates matrices within which to store the equilibrium objects $(v_t, k'_t, W_t)_{t=0}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_val = 1\n",
    "\n",
    "# set up an instance\n",
    "qh = LS_qhyp_cons(alpha = 0.4,\n",
    "                beta= beta_val,\n",
    "                delta = 0.96,\n",
    "                mu = 0,\n",
    "                s = 0.1,\n",
    "                grid_max = 4,\n",
    "                grid_size = 100,\n",
    "                shock_size = 250,\n",
    "                T = 4)\n",
    "\n",
    "# Value function at time T which initiates the backward induction (it's the utility of all possible output values)\n",
    "W_T = qh.u(qh.grid)         \n",
    "\n",
    "# Record the T continuation value functions discounting with factor delta\n",
    "W = np.empty((int(qh.T), len(W_T)))\n",
    "W[qh.T-1,:] = W_T                # last value function is just the utility of all possible output values\n",
    "\n",
    "# Record the T policy functions of self t at t\n",
    "k_prime = np.empty((qh.T, len(W_T)))\n",
    "k_prime[qh.T-1,:] = np.zeros(len(W_T))\n",
    "\n",
    "# Record the T value functions of self t at t\n",
    "V = np.empty((qh.T, len(W_T)))\n",
    "V[qh.T-1,:] = W_T                # last value function is just the utility of all possible output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell computes an equilibrium and fills in the matrices from the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = qh.T\n",
    "for i in range(num_iter):\n",
    "    vkw = v_k_w(qh, W[T-i,:])\n",
    "    k_prime[qh.T-1,:] = \n",
    "    W[qh.T-1,:] = vkw[3,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code with analytical solution when there is no hyperbolic discounting (i.e. $\\beta = 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exact solution if there is no hyborlic discounting (i.e. $\\beta=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exact_policy(qh, y):\n",
    "    \"\"\"\n",
    "    True optimal policy\n",
    "    \"\"\"\n",
    "    return (1 - qh.alpha * qh.delta) * y\n",
    "\n",
    "def Exact_v(qh, y):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - qh.alpha * qh.delta) / (1 - qh.delta)\n",
    "    c2 = (qh.mu + qh.alpha * np.log(qh.alpha * qh.delta)) / (1 - qh.alpha)\n",
    "    c3 = 1 / (1 - qh.delta)\n",
    "    c4 = 1 / (1 - qh.alpha * qh.delta)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no hyperbolic discounting (i.e. $\\beta = 1$), solving for the sequence of value and policy functions by backward induction is the same algorithm as Value Function Iteration for an infinite horizon. For an infinite horizon, we are interested in the fixed point of the Bellman equation. The sequence of value functions along the iteration to convergence is not relevant, only the fixed point matters. For a finite horizon, the sequence of value functions along the iteration is the object of interest; each iteration solves for the previous value function and the previous policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qh = LS_qhyp_cons(alpha = 0.4,\n",
    "                beta= 1,\n",
    "                delta = 0.96,\n",
    "                mu = 0,\n",
    "                s = 0.1,\n",
    "                grid_max = 4,\n",
    "                grid_size = 100,\n",
    "                shock_size = 250)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "xgrid = qh.grid\n",
    "ax.plot(xgrid, Exact_v(qh, xgrid), label='v')\n",
    "vkw = v_k_w(qh, Exact_v(qh, xgrid))\n",
    "WW = vkw[0,:]\n",
    "ax.plot(xgrid, WW, label='v_k_w(v)')\n",
    "W =  np.log(qh.grid)\n",
    "num_iter = 50\n",
    "for i in range(num_iter):\n",
    "    vkw = v_k_w(qh, W)\n",
    "    W = vkw[2,:]\n",
    "    ax.plot(xgrid, W, color=plt.cm.jet(i / num_iter), lw=2, alpha=0.6)\n",
    "ax.set_title('Exact solution is a fixed point of Bellman operator (no hyperbolic discounting)') \n",
    "ax.set_xlabel('y', fontsize=16)\n",
    "ax.set_ylabel('v', fontsize=16)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
